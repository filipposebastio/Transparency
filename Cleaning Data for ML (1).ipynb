{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Modelling Tool for UAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpu\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################please change finding name!!!!!!!!!!!!!!!!!!!!!!##################################\n",
    "finding='Unauthorized Sub-Contracting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for re-grouping null value or other 'Product type' in 'Quantitative_Data'\n",
    "pdt_type_list=['shoe','technology','plastic','printing','electronic','packaging','mill','leather','metal','electric','appliance','goods','stationery','sport','paper','optic']\n",
    "apparel_list=['textil','garment','fashion','knit','apparel','craft','cloth']\n",
    "\n",
    "##############Unique column##############\n",
    "#Check unique value for each column, and to see if any column with blank value > 20%. for reference only.\n",
    "def unique_counts(df):\n",
    "    for i in df.columns:\n",
    "        count = df[i].nunique()\n",
    "        null = df[i].isnull().sum()\n",
    "        print(i, \"-\", \"unique count : \", count, \", blank count : \", null, \"(\", '{:,.2%}'.format(null/ df.shape[0]), \")\")\n",
    "        if null/ df.shape[0] > 0.2:\n",
    "            print('==========Blank % > 20%==========', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import FX file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use FX Rate Monthly Average to standardize the currency related field, mainly for 'Quantitative_Data' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_rate=pd.read_excel(r'C:\\Users\\filippo.sebastio\\ELEVATE\\Vignesh Venkataraman - Analytics\\References\\FX rate\\FX Rate Monthly Average.xlsx')\n",
    "fx_rate['Version Month']=fx_rate['Version Month'].astype(str)"
    "###test change###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fx_rate_19 = fx_rate[fx_rate['Year'] == 2019]\n",
    "fx_rate_19 = fx_rate_19.groupby('Country').mean().reset_index()\n",
    "fx_rate_19 = fx_rate_19[['Country', 'USD equivalent']]\n",
    "fx_rate_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Finding file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all 'Report No' with 'Unauthorized Sub-Contracting' Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finding = pd.read_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\EXTRACT__Findings_Data_crosstab(15-16-17-18-Jul 19).xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_finding['argrc__End_Date__c'] = pd.to_datetime(df_finding['argrc__End_Date__c'],format='%d-%b-%Y').dt.strftime('%Y%m')\n",
    "# df_finding.rename(columns={'argrc__End_Date__c': 'Version Month'}, inplace=True)\n",
    "\n",
    "#select only 'Version Month','Report No','Finding name' column, and 'Finding name' = selected finding in cell3 \n",
    "df_finding=df_finding[['argrc__End_Date__c','Report No','Finding name']]\n",
    "df_unauthorized_sub_contract=df_finding[df_finding['Finding name'] == finding]\n",
    "df_unauthorized_sub_contract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Change\n",
    "Introduction of normalized violation variable  vs non normalized (before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. replace all 'NA' like field, incl 'N/A', '#REF!' to np.NaN\n",
    "2. Change date format to YYYYMM for easy reading\n",
    "3a. replace 'Product type' from null to 'other'\n",
    "3b. change 'Product type' from 'other' to specify product type(grouping according to cell 4) by consider their factory name\n",
    "4. correct cells from 0 to nan (incl 'Number of findings (total)', 'Hourly wage (legal)','Evt_Latitude__c', 'Evt_Longitude__c') as they are wrongly input / for further use ('Number of findings (total)' case)\n",
    "5. drop records without figure, from 'Daily hours (highest)' to 'Hourly wage (average)' = null\n",
    "6. Remove unreasonable record, e.g %>100, average>highest, lowest>average, Hourly wage=18029 in TW, Hourly wage=0.35 or >1000 in CN\n",
    "7. change local currecy to USD\n",
    "8. Remove 'Cyprus' and 'Norway' as they have only 1 or 2 audit record and with many null value\n",
    "9. re-order column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quant = pd.read_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\EXTRACT__Quantitative_Data_crosstab (2015-2019)_update.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_quantitative = df_quant.copy()\n",
    "df_quantitative=df_quantitative.replace('N/A',np.NaN)\n",
    "df_quantitative=df_quantitative.replace('#REF!',np.NaN)\n",
    "\n",
    "# df_quantitative['argrc__End_Date__c']=pd.to_datetime(df_quantitative['argrc__End_Date__c'],format='%d/%b/%Y').dt.strftime('%Y%m')\n",
    "\n",
    "#################Data Cleaning#################\n",
    "df_quantitative.loc[df_quantitative['Product type'].isnull(), 'Product type'] = 'other'\n",
    "df_quantitative.loc[df_quantitative['Number of findings (total)'] == 0, 'Number of findings (total)'] = np.nan\n",
    "df_quantitative.loc[df_quantitative['Hourly wage (legal)'] == 0, 'Hourly wage (legal)'] = np.nan\n",
    "\n",
    "df_quantitative.loc[df_quantitative['Evt_Latitude__c'] == 0, 'Evt_Latitude__c'] = np.nan\n",
    "df_quantitative.loc[df_quantitative['Evt_Longitude__c'] == 0, 'Evt_Longitude__c'] = np.nan\n",
    "df_quantitative['Product type']=df_quantitative['Product type'].str.lower()\n",
    "df_quantitative['Factory name']=df_quantitative['Factory name'].str.lower()\n",
    "#################End of Data Cleaning#################\n",
    "\n",
    "#drop records without figure\n",
    "#i.e. 'Daily hours (highest)' to 'Hourly wage (average)' = null\n",
    "df_quantitative_audit_only=df_quantitative.dropna(subset = ['Daily hours (highest)','Weekly hours (average)','Weekly hours (highest)','Continuous days (average)','Continuous days (most)','Hourly wage (lowest)','Hourly wage (average)'], how='all')\n",
    "\n",
    "#Keep only % <= 100 records\n",
    "df_quantitative_audit_only=df_quantitative_audit_only[df_quantitative_audit_only['Paid above monthly (%)']<= 100]\n",
    "\n",
    "# Hourly wage (legal): TW remove 18029, CN remove 0.35 and >1000\n",
    "df_quantitative_audit_only=df_quantitative_audit_only.drop(df_quantitative_audit_only[(df_quantitative_audit_only['Country'] == 'Taiwan')&((df_quantitative_audit_only['Hourly wage (legal)']>10000)|(df_quantitative_audit_only['Hourly wage (lowest)']>10000))].index)\n",
    "df_quantitative_audit_only=df_quantitative_audit_only.drop(df_quantitative_audit_only[(df_quantitative_audit_only['Country'] == 'China')&((df_quantitative_audit_only['Hourly wage (legal)']>1000)|(df_quantitative_audit_only['Hourly wage (legal)']<1))].index)\n",
    "\n",
    "#Keep only reasonable records\n",
    "df_quantitative_audit_only=df_quantitative_audit_only[df_quantitative_audit_only['Weekly hours (average)']<=df_quantitative_audit_only['Weekly hours (highest)']]\n",
    "df_quantitative_audit_only=df_quantitative_audit_only[df_quantitative_audit_only['Continuous days (average)']<=df_quantitative_audit_only['Continuous days (most)']]\n",
    "df_quantitative_audit_only=df_quantitative_audit_only[df_quantitative_audit_only['Hourly wage (lowest)']<=df_quantitative_audit_only['Hourly wage (average)']]\n",
    "df_quantitative_audit_only=df_quantitative_audit_only[df_quantitative_audit_only['Monthly take-home (lowest)']<=df_quantitative_audit_only['Monthly take-home (average)']]\n",
    "\n",
    "#change 'Product type' from 'other' to specify product type by consider their factory name\n",
    "df_quantitative_audit_only.loc[((df_quantitative_audit_only['Product type']=='other')|\\\n",
    "                                   (df_quantitative_audit_only['Product type'].isnull()))&\\\n",
    "                                   (df_quantitative_audit_only['Factory name'].isin(apparel_list)), 'Product type']='apparel'\n",
    "\n",
    "for item in pdt_type_list:\n",
    "    \n",
    "    df_quantitative_audit_only.loc[((df_quantitative_audit_only['Product type']=='other')|\\\n",
    "                                   (df_quantitative_audit_only['Product type'].isnull()))&\\\n",
    "                                   (df_quantitative_audit_only['Factory name'].str.contains(item)), 'Product type']=item \n",
    "\n",
    "#change name    \n",
    "df_quantitative_audit_only.rename(columns={'argrc__End_Date__c': 'Version Month'}, inplace=True)\n",
    "\n",
    "#chage currency\n",
    "#'Hourly wage (lowest)','Hourly wage (legal)','Hourly wage (average)','Monthly take-home (lowest)','Monthly take-home (average)'\n",
    "    \n",
    "df_quantitative_audit_only = pd.merge(df_quantitative_audit_only, fx_rate_19, on=['Country'], how='left')\n",
    "\n",
    "df_quantitative_audit_only['Hourly wage_above_legal_%']=df_quantitative_audit_only['Hourly wage (average)']/df_quantitative_audit_only['Hourly wage (legal)']-1\n",
    "\n",
    "df_quantitative_audit_only['Hourly wage (lowest)_USD equ']=df_quantitative_audit_only['Hourly wage (lowest)']*df_quantitative_audit_only['USD equivalent']\n",
    "df_quantitative_audit_only['Hourly wage (legal)_USD equ']=df_quantitative_audit_only['Hourly wage (legal)']*df_quantitative_audit_only['USD equivalent']\n",
    "df_quantitative_audit_only['Hourly wage (average)_USD equ']=df_quantitative_audit_only['Hourly wage (average)']*df_quantitative_audit_only['USD equivalent']\n",
    "df_quantitative_audit_only['Monthly take-home (lowest)_USD equ']=df_quantitative_audit_only['Monthly take-home (lowest)']*df_quantitative_audit_only['USD equivalent']\n",
    "df_quantitative_audit_only['Monthly take-home (average)_USD equ']=df_quantitative_audit_only['Monthly take-home (average)']*df_quantitative_audit_only['USD equivalent']\n",
    "\n",
    "#drop 'Cyprus' and 'Norway'\n",
    "df_quantitative_audit_only = df_quantitative_audit_only[(df_quantitative_audit_only['Country'] != 'Cyprus')|(df_quantitative_audit_only['Country'] != 'Norway')]\n",
    "\n",
    "#re-order column\n",
    "df_quantitative_audit_only = df_quantitative_audit_only[['Report No','Version Month', 'Factory name', 'ISO_Region', \\\n",
    "                                                         'Country', 'Province', 'Evt_Latitude__c','Evt_Longitude__c', \\\n",
    "                                                         'Program', 'Product type','Service__c', 'Buyer 1 (%)',\\\n",
    "                                                         'Number of workers', 'Daily hours (highest)', 'Weekly hours (average)',\\\n",
    "                                                         'Weekly hours (highest)', 'Continuous days (average)','Continuous days (most)',\\\n",
    "                                                         'Hourly wage_above_legal_%',\\\n",
    "                                                         'Hourly wage (lowest)_USD equ','Hourly wage (legal)_USD equ','Hourly wage (average)_USD equ',\\\n",
    "                                                         'Paid above hourly (%)', 'Paid correctly (%)','Paid above monthly (%)', \\\n",
    "                                                    \n",
    "                                                         'Monthly take-home (lowest)_USD equ','Monthly take-home (average)_USD equ',\\\n",
    "                                                         'Business Ethics Findings', 'Environment Findings',\\\n",
    "                                                         'Health & Safety Findings', 'Labour Standards Findings',\\\n",
    "                                                         'Management Systems Findings', 'Transparency',\n",
    "                                                        'Number of findings (total)' \n",
    "                                                        ]]\n",
    "\n",
    "\n",
    "scores = ['Business Ethics Findings',\n",
    "       'Environment Findings', 'Health & Safety Findings',\n",
    "       'Labour Standards Findings', 'Management Systems Findings', 'Number of findings (total)' ]\n",
    "\n",
    "for i in scores:\n",
    "    stats = df_quantitative_audit_only.groupby('Service__c')[i].agg(['min','max', 'std', 'mean']).reset_index()\n",
    "    df_quantitative_audit_only =  pd.merge(df_quantitative_audit_only, stats, on = 'Service__c', how = 'left')\n",
    "    df_quantitative_audit_only[i + ' Normalized'] =  (df_quantitative_audit_only[i]-df_quantitative_audit_only['min'])/(df_quantitative_audit_only['max']-df_quantitative_audit_only['min'])\n",
    "    df_quantitative_audit_only = df_quantitative_audit_only.drop(['min', 'max','std', 'mean'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Quantitative_Data-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create 'Union or Worker_Committee' table\n",
    "2. select 'Report No','Union_Name','CBA_Valid_To','Worker_Committee_Details' column\n",
    "3. create 'language' table\n",
    "4. Data Cleaning for symbol, spelling mistake, inconsistent input, missing or redundant punctuation mark for free text columns\n",
    "5. create 'same_language' column to check if Management and Workers are speaking the same language, however, there are too many blank records, you can uncomment the code for df_language if you still want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q_2=pd.read_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\EXTRACT__Quantitative_Data-2_crosstab (2015- Jul 2019).xlsx',na_values=['N/A','-','M/A','N.A','N//A','N/A','N/A no relevant local law','NA','NA, there is no Union','Ni/A','Nil','Nil.','No','None','Not Applicable','Not appllicable','Not Available'])\n",
    "\n",
    "df_quantitative_2 = df_q_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Union or Worker_Committee\n",
    "df_union = df_quantitative_2[['Report No','Union_Name','CBA_Valid_To','Worker_Committee_Details']]\n",
    "df_union = df_union.dropna(subset = ['Union_Name','CBA_Valid_To','Worker_Committee_Details'])\n",
    "\n",
    "# #################################################################################################################\n",
    "df_language = df_quantitative_2[['Report No','Management_Language','Workers_Language','Owner_Nationality']]\n",
    "\n",
    "# #################Data Cleaning#################\n",
    "df_language['Management_Language'].replace('[-]', ',', regex=True, inplace=True)\n",
    "df_language['Workers_Language'].replace('[-]', ',', regex=True, inplace=True)\n",
    "df_language['Management_Language']=df_language['Management_Language'].str.lower()\n",
    "df_language['Workers_Language']=df_language['Workers_Language'].str.lower()\n",
    "\n",
    "df_language.replace('[&/.;()]', ',', regex=True, inplace=True)\n",
    "df_language.replace(' and ', ', ', regex=True, inplace=True)\n",
    "\n",
    "df_language.replace('chinse', 'chinese', regex=True, inplace=True)\n",
    "df_language.replace('chiinese', 'chinese', regex=True, inplace=True)\n",
    "df_language.replace('chineses', 'chinese', regex=True, inplace=True)\n",
    "df_language.replace('chiese', 'chinese', regex=True, inplace=True)\n",
    "\n",
    "df_language.replace('bahasa indonesia english', 'bahasa indonesia, english', regex=True, inplace=True)\n",
    "df_language.replace('indonesian', 'indonesia', regex=True, inplace=True)\n",
    "\n",
    "df_language.replace('cammbodia', 'cambodia', regex=True, inplace=True)\n",
    "df_language.replace('cammbodian', 'cambodia', regex=True, inplace=True)\n",
    "df_language.replace('cambodian english', 'cambodia, english', regex=True, inplace=True)\n",
    "df_language.replace('cambodian chinese', 'cambodia, chinese', regex=True, inplace=True)\n",
    "df_language.replace('khmer', 'cambodia', regex=True, inplace=True)\n",
    "df_language.replace('cambodian', 'cambodia', regex=True, inplace=True)\n",
    "\n",
    "df_language.replace('english urdu punjabi', 'english, urdu, punjabi', regex=True, inplace=True)\n",
    "df_language.replace('english urdu', 'english, urdu', regex=True, inplace=True)\n",
    "df_language.replace('urdu punjabi', 'urdu, punjabi', regex=True, inplace=True)\n",
    "df_language.replace('urdu punjab', 'urdu, punjabi', regex=True, inplace=True)\n",
    "df_language.replace('punjabi urdu', 'punjabi, urdu', regex=True, inplace=True)\n",
    "\n",
    "df_language.replace('tami ,', 'tamil,', regex=True, inplace=True)\n",
    "df_language.replace('bangali', 'bengali', regex=True, inplace=True)\n",
    "df_language.replace('potuguese', 'portuguese', regex=True, inplace=True)\n",
    "df_language.replace('viettnamese', 'vietnamese', regex=True, inplace=True)\n",
    "df_language.replace('mnadarin', 'mandarin', regex=True, inplace=True)\n",
    "df_language.replace('english dutch', 'english, dutch', regex=True, inplace=True)\n",
    "df_language.replace('english dutch french', 'english, dutch, french', regex=True, inplace=True)\n",
    "df_language.replace('different account to their country language but english , hindi was common', 'english, hindi', regex=True, inplace=True)\n",
    "# #################End of Data Cleaning#################\n",
    "\n",
    "\n",
    "\n",
    "#df_language.head()\n",
    "\n",
    "df_language['Owner_Nationality'] = df_language['Owner_Nationality'].str.strip()\n",
    "\n",
    "re = {'China':'Chinese', 'Korea':'Korean', 'Taiwan': 'Taiwanese', 'USA':'American', 'Indonesia':'Indonesian',\n",
    "     'Pakistan':'Pakistani', 'India':'Indian', 'Bangladesh':'Bangladeshi', 'Japan':'Japanese',\n",
    "     'Viet Nam':'Vietnamese', 'Sri Lanka':'Sri Lankan', 'TURKISH':'Turkish', 'Thailand':'Thai',\n",
    "     'Mexico':'Mexican', 'Korea, Republic of':'South Korean', 'Germany':'German', 'Hongkong':'Hong Kong', 'Brazil':'Brazilian',\n",
    "     'Portuguse':'Portuguese', 'Mexicans':'Mexican', 'Indians':'India', 'Canada':'Canadian', \n",
    "     'Portugal': 'Portuguese', 'South Korea':'South Korean',  'Vietnam':'Vietnamese' , 'Americans':'American',\n",
    "     'Korean':'South Korean', 'Turkey':'Turkish', 'India':'Indian', 'Malaysia':'Malaysian', 'Hongkong':'HongKong'}\n",
    "\n",
    "df_language['Owner_Nationality'] = df_language['Owner_Nationality'].map(re).fillna(df_language['Owner_Nationality'])\n",
    "\n",
    "\n",
    "dn = pd.read_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\denomyns.xlsx')\n",
    "df_language = pd.merge(df_language, dn, left_on = 'Owner_Nationality', right_on = 'Nationality', indicator = True, how ='left')\n",
    "\n",
    "\n",
    "df_language = df_language.dropna()\n",
    "\n",
    "zipped = zip(df_language['Management_Language'], df_language['Workers_Language'])\n",
    "\n",
    "def setify(x):\n",
    "   return set(map(str.strip, filter(None, x.split(','))))\n",
    "\n",
    "#check Management and Workers have same language\n",
    "df_language['same_language'] = [int(bool(setify(a) & setify(b))) for a, b in zipped]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Workers_Demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. check 'Factory country' and 'Workers country of origin' to see if there are migrant worker\n",
    "2. #create 'migrant worker %'\n",
    "3. count for 'Workers country of origin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_demographics= pd.read_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\EXTRACT__Workers_Demographics_crosstab(2015-Sep 2019).xlsx')\n",
    "\n",
    "df_demographics['worker']='migrant'\n",
    "df_demographics.loc[df_demographics['Factory country'] == df_demographics['Workers country of origin'], 'worker'] = 'local'\n",
    "df_demographics['Workers country of origin'].replace('Korean', 'Korea, Republic of', regex=True, inplace=True)\n",
    "\n",
    "worker_distribution=pd.pivot_table(df_demographics, values='Count number of workers', columns=['worker'],index=['Report No'], aggfunc=np.sum).reset_index()\n",
    "worker_distribution.fillna(0, inplace=True)\n",
    "\n",
    "#worker\n",
    "worker_origin_no=df_demographics.groupby(['Report No'])['Workers country of origin'].nunique()\n",
    "\n",
    "#Add migrant worker %\n",
    "worker_distribution['migrant worker %']=worker_distribution['migrant']/(worker_distribution['local']+worker_distribution['migrant'])*100\n",
    "worker_origin_no = worker_origin_no.reset_index()\n",
    "\n",
    "worker_distribution=pd.merge(worker_distribution, worker_origin_no, on='Report No', how='left')\n",
    "\n",
    "worker_distribution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Normalized_Quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.merge(df_quantitative_audit_only, worker_distribution, on='Report No', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create NA list\n",
    "2. bottom part no need to change as result will stay the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NA list (to replace with other number use in cell 13)\n",
    "client_na_list=\\\n",
    "['% are not revealed for privacy',\n",
    " '% not provided',\n",
    " '% not provided due to confidentiality',\n",
    " '% not provided due to data protection',\n",
    " '% not revealed due to privacy',\n",
    " '%are not revealed for privacy',\n",
    " '&other stories',\n",
    " '&others stories',\n",
    " '&quot;a&quot; client',\n",
    " '( not provided)',\n",
    " '*owner had the information.',\n",
    " '??',\n",
    " '???',\n",
    " '????',\n",
    " '???? ?own brand)',\n",
    " '´',\n",
    " 'confedential',\n",
    " 'confidencial',\n",
    " 'confidentail',\n",
    " 'confidential',\n",
    " 'confidential - big box retailers',\n",
    " 'confidential – none over',\n",
    " 'confidential (not provided)',\n",
    " 'confidential (other major food, drug and merchandise retailers)',\n",
    " 'confidential 40% government based and 60% consumer based but it can easily change year to year',\n",
    " 'confidential brand',\n",
    " 'confidential brands',\n",
    " 'confidential by customer requirement',\n",
    " 'confidential data',\n",
    " 'confidential info',\n",
    " 'confidential information',\n",
    " 'confidential private labels',\n",
    " 'confidential, low volume – high complexity, engineered to order',\n",
    " 'confidentiality',\n",
    " 'for home – many different brands and for different industries including automotive sector. the names and percentages was not provided due to confidentiality.',\n",
    " 'for independent customers and mall sale',\n",
    " 'for other clients (details of other clients were not provided)',\n",
    " 'na',\n",
    " 'n/a',\n",
    " 'n/a (confidential informaiton)',\n",
    " 'n/a (confidential)',\n",
    " 'n/a (no information provided)',\n",
    " 'n/a because they are not producing for the final client directly.',\n",
    " 'n/a, confidential',\n",
    " 'na',\n",
    " 'na – confidential information for the company',\n",
    " 'nan',\n",
    " 'no data provided',\n",
    " 'no detailed  information provided by  the management',\n",
    " 'no disclosed',\n",
    " 'no disclosed %',\n",
    " 'no excess',\n",
    " 'no info obtained',\n",
    " 'no info obtained due to confidentiality',\n",
    " 'no info provided due to confidentiality',\n",
    " 'no info received due to confidentiality',\n",
    " 'no info received from the facility due to confidentiality',\n",
    " 'no information',\n",
    " 'no information for all clients',\n",
    " 'no information provided',\n",
    " 'no information provided by factory',\n",
    " 'no information provided by the factory',\n",
    " 'no limits',\n",
    " 'no provided',\n",
    " 'no provided by the factory',\n",
    " 'no provided by the factory.',\n",
    " 'no provided due to data protection',\n",
    " 'no provied due to data protection',\n",
    " 'not  provided',\n",
    " 'not  revealed for privacy',\n",
    " 'not applicable',\n",
    " 'not available',\n",
    " 'not available at the audit moment.',\n",
    " 'not available during the assessment',\n",
    " 'not convenient',\n",
    " 'not declared',\n",
    " 'not diclosed to confidentiality agreement.',\n",
    " 'not disclosed',\n",
    " 'not disclosed by factory due to confidentiality',\n",
    " 'not disclosed by the facility',\n",
    " 'not disclosed due to confidential agreement with other client',\n",
    " 'not disclosed due to privacy',\n",
    " 'not disclosed due to privacy agreement',\n",
    " 'not disclosed for confidentiality',\n",
    " 'not disclosed for confidentiality reasons',\n",
    " 'not disclosed for privacy agreement reason',\n",
    " 'not disclosed for privacy reasons',\n",
    " 'not disclosed or % of production',\n",
    " 'not disclosure',\n",
    " 'not disclsed',\n",
    " 'not divulged by the factory due to confidentiality reasons',\n",
    " 'not informed- intern policy',\n",
    " 'not mentioned due to privacy policy',\n",
    " 'not porvided',\n",
    " 'not possible to provide as the visited factory is a subcontractor and does only dyeing-washing process',\n",
    " 'not probided',\n",
    " 'not provdied for confidentiality',\n",
    " 'not provide',\n",
    " 'not provide the brands name',\n",
    " 'not provided',\n",
    " 'not provided - deemed confidential information',\n",
    " 'not provided (confidential information)',\n",
    " 'not provided (confidential terms)',\n",
    " 'not provided (confidential)',\n",
    " 'not provided as confidentiality',\n",
    " 'not provided as responsible person was not present',\n",
    " 'not provided based on confidential reason',\n",
    " 'not provided because confidentiality',\n",
    " 'not provided because confidentiality.',\n",
    " 'not provided because it is managed by head office.',\n",
    " 'not provided by factory',\n",
    " 'not provided by management',\n",
    " 'not provided by the facility',\n",
    " 'not provided by the facility.',\n",
    " 'not provided by the factory',\n",
    " 'not provided by the factory (check data sheet)',\n",
    " 'not provided by the management due to confidentiality',\n",
    " 'not provided due information confidention',\n",
    " 'not provided due to  confidentiality',\n",
    " 'not provided due to business confidentiality',\n",
    " 'not provided due to business secret',\n",
    " 'not provided due to business secret.',\n",
    " \"not provided due to client's confidential policy\",\n",
    " 'not provided due to commercial confidentiality',\n",
    " 'not provided due to confidential',\n",
    " 'not provided due to confidential commitment with other clients',\n",
    " 'not provided due to confidential information',\n",
    " 'not provided due to confidential issues',\n",
    " 'not provided due to confidential policies.',\n",
    " 'not provided due to confidential reason',\n",
    " 'not provided due to confidential requirement',\n",
    " 'not provided due to confidentiality',\n",
    " 'not provided due to confidentiality reasons',\n",
    " 'not provided due to confidentiality reasons with other customers',\n",
    " 'not provided due to confidentiality terms',\n",
    " 'not provided due to confidentiality.',\n",
    " 'not provided due to data protection',\n",
    " 'not provided due to privacy',\n",
    " 'not provided due to privacy policy',\n",
    " 'not provided due to secret',\n",
    " 'not provided due to the company confidential policy',\n",
    " 'not provided even after several requests due to confidentiality',\n",
    " 'not provided for business confidentiality',\n",
    " 'not provided for business confidnetiality',\n",
    " 'not provided for business secret',\n",
    " 'not provided for commercial secret',\n",
    " 'not provided for confidential reason',\n",
    " 'not provided for confidentiality',\n",
    " 'not provided for other client',\n",
    " 'not provided including client production percentage due to confidentiality reasons',\n",
    " 'not provided information',\n",
    " 'not provided.',\n",
    " 'not provided-export market',\n",
    " 'not provied',\n",
    " 'not ptovided',\n",
    " 'not ptovided due to confidentiality',\n",
    " 'not revealed due to privacy',\n",
    " 'not revealed due to privacy reasons',\n",
    " 'not revealed for privacy',\n",
    " 'not revelaed for privacy',\n",
    " 'not revelead due to privacy agreement',\n",
    " 'not shared due to confidentiality',\n",
    " 'other',\n",
    " 'other (confidential)',\n",
    " 'other (confidentiality)',\n",
    " 'others (detailed not provided)',\n",
    " 'other (details not provided due to confidentiality)',\n",
    " 'other (information was not disclosed due to the confidential agreement)',\n",
    " 'other (not provided)',\n",
    " 'other (not shared)',\n",
    " 'other clients are not provided because confidentiality',\n",
    " 'other clients are not revealed due to privacy',\n",
    " 'other clients cannot be disclosed due to privacy',\n",
    " 'other clients cannot be disclosed for privacy reason',\n",
    " 'other clients were not disclosed due to confidentiality',\n",
    " 'other clients were not revealed due to privacy',\n",
    " 'others no disclosed',\n",
    " 'others not provide',\n",
    " 'others not provided',\n",
    " 'others(not provided)',\n",
    " 'others: confidential',\n",
    " 'tbc',\n",
    " 'top secret',\n",
    " 'true',\n",
    " 'name?',\n",
    " '0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Buyers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data cleaning for 'Client_1_Name' to 'Client_10_Name' field, e.g. remove prefix and suffix with ',', remove content if same as client_na_list\n",
    "2. Create 'Client' column to combine content of 'Client_1_Name' to 'Client_10_Name'\n",
    "3. Count the number of ','+1 and '%' (as some free text don't have comma, but show different client %)\n",
    "4. select max of ','+1 and '%' as 'client_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buyer=pd.read_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\EXTRACT__Buyers_crosstab (2015-Sep 2019).xlsx') \n",
    "\n",
    "client_name_col=['Client_1_Name','Client_2_Name','Client_3_Name','Client_4_Name','Client_5_Name','Client_6_Name','Client_7_Name','Client_8_Name','Client_9_Name','Client_10_Name']\n",
    "\n",
    "######data cleaning######\n",
    "\n",
    "for name in client_name_col:\n",
    "    df_buyer[name]=df_buyer[name].str.lower()\n",
    "    #remove prefix and suffix with ','\n",
    "    df_buyer.loc[(~df_buyer[name].isnull())&(df_buyer[name].str.startswith(',')), name] = df_buyer[name].str[1:]\n",
    "    df_buyer.loc[(~df_buyer[name].isnull())&(df_buyer[name].str.endswith(',')), name] = df_buyer[name].str[:-1]\n",
    "    #remove content if in client_na_list\n",
    "    df_buyer.loc[(df_buyer[name].isin(client_na_list))|(df_buyer[name]==0), name] = np.nan\n",
    "  \n",
    "df_buyer['Client'] = df_buyer[client_name_col].apply(lambda x: ','.join(x.values.astype(str)), axis=1)\n",
    "df_buyer['Client'].replace(',nan', '', regex=True, inplace=True)\n",
    "df_buyer['Client'].replace('nan', '', regex=True, inplace=True)\n",
    "df_buyer['Client'].replace('&amp;', '', regex=True, inplace=True)\n",
    "df_buyer['Client'].replace('[、;/]', '', regex=True, inplace=True)\n",
    "\n",
    "df_buyer.loc[df_buyer['Client']=='Info not obtained from the facility due to confidentiality. Biggest markets are Germany and Scandinavian countries.', 'Client'] = 'Germany, Scandinavian'\n",
    "\n",
    "df_buyer['comma']=df_buyer['Client'].str.count(',')+1\n",
    "df_buyer.loc[(df_buyer['Client']=='')|(df_buyer['Client']=='no other brand')|(df_buyer['Client']=='no clients as yet')\\\n",
    "             |(df_buyer['Client']=='N/A. The factory started operations in Oct 2016, and have not received any orders. Currently, workers are being trained by the factory.'), 'comma'] = 0\n",
    "\n",
    "df_buyer['percent']=df_buyer['Client'].str.count('%')\n",
    "df_buyer['client_count']=df_buyer[['comma', 'percent']].max(axis=1)\n",
    "\n",
    "df_buyer=df_buyer[['Report No','client_count']]\n",
    "df_buyer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df1, df_buyer, on='Report No', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_selected = df_language[['Report No', 'Owner Country', 'same_language']]\n",
    "df3 = pd.merge(df2, df_language_selected, on='Report No', how='left')\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Factory_Certifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.09.19 --- continue from here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_factory_certc= pd.read_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\EXTRACT__Factory_Certifications_crosstab (2015-Sep2019).xlsx', na_values=['No','no','None','0'],index_col=0)\n",
    "df_factory_certc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum up and count all the column endswith '_Approved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_factory_certc = df_factory_certc.reset_index()\n",
    "df_factory_cert = df_factory_certc.copy()\n",
    "df_factory_cert =  df_factory_cert.set_index('Report No')\n",
    "df_factory_cert=df_factory_cert.loc[:, df_factory_cert.columns.str.endswith('_Approved')]\n",
    "df_factory_cert = df_factory_cert.reset_index()\n",
    "df_factory_cert.fillna(0, inplace=True)\n",
    "df_factory_cert.replace('Yes', 1, regex=True, inplace=True)\n",
    "\n",
    "cl = ['BAP_Approved', 'BetterWork_Approved', 'BSCI_Approved', 'EICC_Approved',\n",
    "       'ISO14001_Approved', 'OHSAS18001_Approved', 'SA8000_Approved',\n",
    "       'SMETA_Approved', 'WRAP_Approved']\n",
    "\n",
    "for col in cl:\n",
    "    df_factory_cert['col'] = pd.to_numeric(df_factory_cert[col], errors = 'coerce')\n",
    "\n",
    "df_factory_cert['cert_count']=df_factory_cert.sum(axis=1,skipna=False)\n",
    "df_factory_cert.reset_index(inplace=True)\n",
    "df_factory_cert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create 'Unauthorized Sub-Contracting' and set to 1 if df['Report No'] in df_unauthorized_sub_contract['Report No']\n",
    "2. #Remove 'Service__c' if the group has null value for all record\n",
    "3. create 'Union_Worker_Committee' and set to Y if df['Report No'] in df_union['Report No']\n",
    "4. df['Transparency'] == 'Transparent' only as the others are fake records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Change number two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering both transparent and non transparent factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df4 = pd.merge(df3, df_factory_cert, on='Report No', how='left')\n",
    "df4['Unauthorized Sub-Contracting']=0\n",
    "df4.loc[(df4['Report No'].isin(df_unauthorized_sub_contract['Report No'])), 'Unauthorized Sub-Contracting']=1\n",
    "df4['Unauthorized Sub-Contracting']=df4['Unauthorized Sub-Contracting'].astype(int)\n",
    "\n",
    "# #Removed 'Service__c'\n",
    "df_service_count = df4.groupby(['Service__c'])['Unauthorized Sub-Contracting'].sum().sort_values(ascending=False).reset_index()\n",
    "incl_service_list=df_service_count[df_service_count['Unauthorized Sub-Contracting']>0]['Service__c'].tolist()\n",
    "\n",
    "df4['Union_Worker_Committee']='N'\n",
    "df4.loc[(df4['Report No'].isin(df_union['Report No'])), 'Union_Worker_Committee']='Y' \n",
    "\n",
    "\n",
    "# df4=df4[(df4['Service__c'].isin(incl_service_list))&(df['Transparency'] == 'Transparent')]\n",
    "#df.dropna(subset = ['same_language'],inplace=True)\n",
    "\n",
    "df4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Version Month'] = pd.to_datetime(df4['Version Month'])\n",
    "df4['year'] = pd.DatetimeIndex(df4['Version Month']).year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Change from Previous Version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**assign median value to df_0 as well if variable is missing.**\n",
    "\n",
    "1. split file depending if they their 'Unauthorized Sub-Contracting' or not\n",
    "2. Select not null data for 'Unauthorized Sub-Contracting'=0 records\n",
    "2. Replace null value with country median for 'Unauthorized Sub-Contracting'=1 records\n",
    "4. concat them as df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df4.fillna(df4.groupby(['Country', 'Province'])['Buyer 1 (%)','Number of workers',\\\n",
    "#                       'Daily hours (highest)','Weekly hours (highest)',\\\n",
    "#                       'Hourly wage (legal)','Continuous days (most)','Paid above hourly (%)','Paid correctly (%)',\\\n",
    "#                       'Paid above monthly (%)','Monthly take-home (average)',\\\n",
    "#                       'Monthly take-home (average)',\\\n",
    "#                       'Business Ethics Findings','Environment Findings','Health & Safety Findings',\\\n",
    "#                       'Labour Standards Findings','Management Systems Findings','migrant worker %',\\\n",
    "#                       'Workers country of origin',\n",
    "#                       'client_count','cert_count','same_language'].transform('median'), inplace=True)\n",
    "\n",
    "df_0=df4[df4['Unauthorized Sub-Contracting']==0]\n",
    "df_1=df4[df4['Unauthorized Sub-Contracting']==1]\n",
    "\n",
    "# Select clean data for 'Unauthorized Sub-Contracting'=0 records\n",
    "# df_0.dropna(subset = ['Buyer 1 (%)','Number of workers',\\\n",
    "#                       'Daily hours (highest)','Weekly hours (highest)',\\\n",
    "#                       'Hourly wage (legal)','Continuous days (most)','Paid above hourly (%)','Paid correctly (%)',\\\n",
    "#                       'Paid above monthly (%)','Monthly take-home (average)',\\\n",
    "#                       'Monthly take-home (average)',\\\n",
    "#                       'Business Ethics Findings','Environment Findings','Health & Safety Findings',\\\n",
    "#                       'Labour Standards Findings','Management Systems Findings','migrant worker %',\\\n",
    "#                       'Workers country of origin',\n",
    "#                       'client_count','cert_count','same_language'\n",
    "#                       ], how='any',inplace=True)\n",
    "\n",
    "\n",
    "# Replace null value with country median for 'Unauthorized Sub-Contracting'=1 records\n",
    "df_0.fillna(df_0.groupby('Country').transform('median'), inplace=True)\n",
    "df_0.fillna(df_0.median(), inplace=True)\n",
    "df_0.loc[df_0['same_language'].isnull(), 'same_language']='Y'\n",
    "\n",
    "\n",
    "# Replace null value with country median for 'Unauthorized Sub-Contracting'=1 records\n",
    "df_1.fillna(df_1.groupby('Country').transform('median'), inplace=True)\n",
    "df_1.fillna(df_1.median(), inplace=True)\n",
    "df_1.loc[df_1['same_language'].isnull(), 'same_language']='Y'\n",
    "\n",
    "df_final=pd.concat([df_0, df_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Version Month'] = pd.to_datetime(df_final['Version Month'])\n",
    "df_final['year'] = pd.DatetimeIndex(df_final['Version Month']).year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximity: slice df into country level\n",
    "Change for relevant country\n",
    "1. Select specific country and industry from cell 20\n",
    "2. calculate 'Facory Size'according to 'Number of workers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_2 = {'Vietnam':'Viet Nam', 'Burma':'Myanmar', 'South Korea': 'Korea, Republic of',  'North America':'United States'}\n",
    "df_final['Owner Country'] = df_final['Owner Country'].map(re_2).fillna(df_final['Owner Country'])\n",
    "df_final['Local Owner'] = np.where(df_final['Owner Country'] == df_final['Country'],1,0)\n",
    "\n",
    "df=df_final[['Report No', 'Version Month', 'Factory name', 'ISO_Region', 'Country',\n",
    "       'Province', 'Evt_Latitude__c', 'Evt_Longitude__c', 'Program',\n",
    "       'Product type', 'Service__c', 'Buyer 1 (%)', 'Number of workers',\n",
    "       'Daily hours (highest)', 'Weekly hours (average)',\n",
    "       'Weekly hours (highest)', 'Continuous days (average)',\n",
    "       'Continuous days (most)', 'Hourly wage_above_legal_%',\n",
    "       'Hourly wage (lowest)_USD equ', 'Hourly wage (legal)_USD equ',\n",
    "       'Hourly wage (average)_USD equ', 'Paid above hourly (%)',\n",
    "       'Paid correctly (%)', 'Paid above monthly (%)',\n",
    "       'Monthly take-home (lowest)_USD equ',\n",
    "       'Monthly take-home (average)_USD equ', 'Business Ethics Findings',\n",
    "       'Environment Findings', 'Health & Safety Findings',\n",
    "       'Labour Standards Findings', 'Management Systems Findings',\n",
    "       'Transparency', 'Number of findings (total)',\n",
    "       'Business Ethics Findings Normalized',\n",
    "       'Environment Findings Normalized',\n",
    "       'Health & Safety Findings Normalized',\n",
    "       'Labour Standards Findings Normalized',\n",
    "       'Management Systems Findings Normalized',\n",
    "       'Number of findings (total) Normalized', 'local', 'migrant',\n",
    "       'migrant worker %', 'Workers country of origin', 'year', 'client_count',\n",
    "       'Owner Country', 'same_language', 'cert_count',\n",
    "       'Unauthorized Sub-Contracting', 'Union_Worker_Committee',\n",
    "       'Local Owner']]\n",
    "\n",
    "####Change for Country\n",
    "df = df[(df['Country'] == 'India') & ((df['Product type'] == 'apparel') | (df['Product type'] == 'apparel accessories'))]\n",
    "df = df.dropna(subset=['Evt_Latitude__c'])\n",
    "\n",
    "norm_scores = ['Business Ethics Findings Normalized', 'Labour Standards Findings Normalized', \n",
    "              'Management Systems Findings Normalized', 'Number of findings (total) Normalized', \n",
    "               'Environment Findings Normalized', 'Health & Safety Findings Normalized']\n",
    "for ns in norm_scores:\n",
    "    df[ns] = df[ns]*100\n",
    "    df[ns] = df[ns].round(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters change: Import Factory location (country level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. input country according to the country in cell 20 as df_location1\n",
    "2. Input Quantitative_Data and filter country, industry in cell 20, with 'Factory name' equals to df_location1['Factory name']  as df_location2\n",
    "3. concat the 2 files\n",
    "4. Store df_location['lat'] and df_location['lng'] ino list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oar =pd.read_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\OAR\\OAR_ALL.xlsx')\n",
    "oar_df = oar[oar['country'] == 'India']\n",
    "oar_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################please select country and industry!!!!!!!!!!!!!!!!!!!!!!!!##################################\n",
    "#for use in cell 22 and 23\n",
    "country='India'\n",
    "industry=['apparel', 'apparel accessories'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latitude/Longitude Distance Calculator\n",
    "#https://openapparel.org/\n",
    "\n",
    "df_location1 = oar_df.copy()\n",
    "\n",
    "df_location1.rename(columns={'name': 'Factory name'}, inplace=True)\n",
    "df_location1['Factory name']=df_location1['Factory name'].str.lower()\n",
    "\n",
    "df_location2= df.copy()\n",
    "\n",
    "df_location2['Factory name']=df_location2['Factory name'].str.lower()\n",
    "\n",
    "df_location2=df_location2[(df_location2['Country']==country)&\\\n",
    "                          (df_location2['Evt_Latitude__c']!=0)&\\\n",
    "                          (df_location2['Evt_Longitude__c']!=0)&\\\n",
    "                          (df_location2['Product type'].isin(industry))&\\\n",
    "                          (~df_location2['Factory name'].isin(df_location1['Factory name']))]\n",
    "\n",
    "\n",
    "df_location2.head()\n",
    "df_location2=df_location2[['Report No', 'Factory name','Evt_Latitude__c','Evt_Longitude__c','Number of workers','Business Ethics Findings Normalized', 'Labour Standards Findings Normalized', \n",
    "              'Management Systems Findings Normalized', 'Number of findings (total) Normalized', \n",
    "               'Environment Findings Normalized', 'Health & Safety Findings Normalized', 'Unauthorized Sub-Contracting', 'Monthly take-home (average)_USD equ']]\n",
    "\n",
    "df_location2=df_location2.dropna()\n",
    "\n",
    "df_location2=df_location2.groupby(['Report No', 'Factory name','Evt_Latitude__c','Evt_Longitude__c'])['Number of workers', 'Business Ethics Findings Normalized', 'Labour Standards Findings Normalized', \n",
    "              'Management Systems Findings Normalized', 'Number of findings (total) Normalized', \n",
    "               'Environment Findings Normalized', 'Health & Safety Findings Normalized', 'Unauthorized Sub-Contracting', 'Monthly take-home (average)_USD equ'].mean().reset_index()\n",
    "\n",
    "df_location2=df_location2.rename(columns={'Evt_Latitude__c': 'lat', 'Evt_Longitude__c': 'lng'})\n",
    "\n",
    "df_location2['Factory Size']='Unknown'\n",
    "df_location2.loc[df_location2['Number of workers']<=100,'Factory Size'] = 'Small'\n",
    "df_location2.loc[(df_location2['Number of workers']>100)&(df_location2['Number of workers']<=500),'Factory Size'] = 'Medium'\n",
    "df_location2.loc[df_location2['Number of workers']>500,'Factory Size'] = 'Large'\n",
    "\n",
    "df_location=pd.concat([df_location1, df_location2])\n",
    "df_location=df_location[['Factory name','lat','lng','Factory Size','Business Ethics Findings Normalized', 'Labour Standards Findings Normalized', \n",
    "              'Management Systems Findings Normalized', 'Number of findings (total) Normalized', \n",
    "               'Environment Findings Normalized', 'Health & Safety Findings Normalized', 'Unauthorized Sub-Contracting', 'Monthly take-home (average)_USD equ']]\n",
    "\n",
    "df_location=df_location.drop_duplicates(keep='last')\n",
    "df_location.head()\n",
    "\n",
    "duplicateRowsDF = df_location[df_location.duplicated(['lat', 'lng'])]\n",
    "duplicateRowsDF = duplicateRowsDF.sort_values(by=['lat'])\n",
    "duplicateRowsDF\n",
    "lat_list=df_location['lat'].tolist()\n",
    "lng_list=df_location['lng'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate distance for factories in df_location and df and store the array in df['dist_col']\n",
    "2. calculate no of nearby factory within 10,50,100,200,500km\n",
    "3. Use proximity_100 as base, find the total no of Facory in different size, and the average finding score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_dist(lat,lon):\n",
    "\n",
    "    dist=[]\n",
    "\n",
    "    for i in range(len(lat_list)):\n",
    "        #calculatig distant in km\n",
    "        cal_dist=mpu.haversine_distance((lat_list[i], lng_list[i]), (lat, lon))\n",
    "        dist.append(cal_dist)\n",
    "    return dist\n",
    "\n",
    "def find_proximity_100(input_list):\n",
    "    position=[n for n, i in enumerate(input_list) if (i<100) & (i!=0)]\n",
    "    return position\n",
    "\n",
    "def find_proximity_10(input_list):\n",
    "    position=[n for n, i in enumerate(input_list) if (i<10) & (i!=0)]\n",
    "    return position\n",
    "\n",
    "def find_proximity_50(input_list):\n",
    "    position=[n for n, i in enumerate(input_list) if (i<50) & (i!=0)]\n",
    "    return position\n",
    "\n",
    "def map_factory_size(input_list):\n",
    "    factory_size_list=[]\n",
    "    #if len(input_list)>0:\n",
    "    for item in input_list:\n",
    "        factory_size=df_location.iloc[item]['Factory Size']\n",
    "        factory_size_list.append(factory_size)\n",
    "    return factory_size_list\n",
    "\n",
    "def factory_size_count(input_list):\n",
    "    return input_list.count(size)\n",
    "\n",
    "#Finding score\n",
    "def map_BE_score(input_list):\n",
    "    BE_score_list=[]\n",
    "    for item in input_list:\n",
    "        score=df_location.iloc[item]['Business Ethics Findings Normalized']\n",
    "        BE_score_list.append(score)\n",
    "    return BE_score_list\n",
    "\n",
    "def map_MS_score(input_list):\n",
    "    MS_score_list=[]\n",
    "    for item in input_list:\n",
    "        score=df_location.iloc[item]['Management Systems Findings Normalized']\n",
    "        MS_score_list.append(score)\n",
    "    return MS_score_list\n",
    "\n",
    "def map_LS_score(input_list):\n",
    "    LS_score_list=[]\n",
    "    for item in input_list:\n",
    "        score=df_location.iloc[item]['Labour Standards Findings Normalized']\n",
    "        LS_score_list.append(score)\n",
    "    return LS_score_list\n",
    "\n",
    "\n",
    "def map_ES_score(input_list):\n",
    "    ES_score_list=[]\n",
    "    for item in input_list:\n",
    "        score=df_location.iloc[item]['Environment Findings Normalized']\n",
    "        ES_score_list.append(score)\n",
    "    return ES_score_list\n",
    "\n",
    "def map_HS_score(input_list):\n",
    "    HS_score_list=[]\n",
    "    for item in input_list:\n",
    "        score=df_location.iloc[item]['Health & Safety Findings Normalized']\n",
    "        HS_score_list.append(score)\n",
    "    return HS_score_list\n",
    "\n",
    "def map_Wage_score(input_list):\n",
    "    WG_score_list=[]\n",
    "    for item in input_list:\n",
    "        score=df_location.iloc[item]['Monthly take-home (average)_USD equ']\n",
    "        WG_score_list.append(score)\n",
    "    return WG_score_list\n",
    "\n",
    "\n",
    "def map_US_score(input_list):\n",
    "    US_score_list=[]\n",
    "    for item in input_list:\n",
    "        score=df_location.iloc[item]['Unauthorized Sub-Contracting']\n",
    "        US_score_list.append(score)\n",
    "    return US_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['dist_col'] = df.apply(lambda x: cal_dist(x.Evt_Latitude__c, x.Evt_Longitude__c), axis=1)\n",
    "\n",
    "print('completed distance calculation')\n",
    "\n",
    "# #calculate no of nearby factory within 10,50,100,200,500km 50, 100\n",
    "for k in [10, 50, 100]:  \n",
    "    df['proximity_'+str(k)]= df['dist_col'].apply(lambda x: len([i for i in x if (i < k) & (i!=0)]))\n",
    "    \n",
    "print('completed calculation of N suppliers close in 10 and 100 KM')\n",
    "    \n",
    "df['proximity_100_position']=df['dist_col'].apply(find_proximity_100)\n",
    "df['proximity_10_position']=df['dist_col'].apply(find_proximity_10)\n",
    "df['proximity_50_position']=df['dist_col'].apply(find_proximity_50)\n",
    "\n",
    "print('completed calculation of position of factories in 10 and 100 KM')\n",
    "\n",
    "df['proximity_100_factory_size']=df['proximity_100_position'].apply(map_factory_size)\n",
    "df['proximity_10_factory_size']=df['proximity_10_position'].apply(map_factory_size)\n",
    "df['proximity_50_factory_size']=df['proximity_50_position'].apply(map_factory_size)\n",
    "\n",
    "print('completed calculation of factory in 10, 50 and 100 KM')\n",
    "\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print(start)\n",
    "\n",
    "for size in ['Small', 'Medium', 'Large']:\n",
    "    df['proximity_100_'+size+'_factory']=df['proximity_100_factory_size'].apply(factory_size_count)\n",
    "    df['proximity_10_'+size+'_factory']=df['proximity_10_factory_size'].apply(factory_size_count)\n",
    "    df['proximity_50_'+size+'_factory']=df['proximity_50_factory_size'].apply(factory_size_count)\n",
    "\n",
    "print('Done with avg size')   \n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(end)\n",
    "    \n",
    "df['Monthly take-home (average)_USD equ'] = df['Monthly take-home (average)_USD equ'].astype(int)\n",
    "\n",
    "start_wage = datetime.datetime.now()\n",
    "print('Wage calculation starts', start_wage)\n",
    "\n",
    "df['Wage_Score_100']=df['proximity_100_position'].apply(map_Wage_score)\n",
    "df['Wage_Score_avg_100']=df['Wage_Score_100'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['Wage_Score_10']=df['proximity_10_position'].apply(map_Wage_score)\n",
    "df['Wage_Score_avg_10']=df['Wage_Score_10'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['Wage_Score_50']=df['proximity_50_position'].apply(map_Wage_score)\n",
    "df['Wage_Score_avg_50']=df['Wage_Score_50'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "print('Done with salary')  \n",
    "\n",
    "end_wage = datetime.datetime.now()\n",
    "print('Wage calculation ends', end_wage)\n",
    "\n",
    "df['BE_Score_100']=df['proximity_100_position'].apply(map_BE_score)\n",
    "df['BE_Score_avg_100']=df['BE_Score_100'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['BE_Score_10']=df['proximity_10_position'].apply(map_BE_score)\n",
    "df['BE_Score_avg_10']=df['BE_Score_10'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['BE_Score_50']=df['proximity_50_position'].apply(map_BE_score)\n",
    "df['BE_Score_avg_50']=df['BE_Score_50'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "print('Done with BE')  \n",
    "\n",
    "df['MS_Score_100']=df['proximity_100_position'].apply(map_MS_score)\n",
    "df['MS_Score_avg_100']=df['MS_Score_100'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['MS_Score_10']=df['proximity_10_position'].apply(map_MS_score)\n",
    "df['MS_Score_avg_10']=df['MS_Score_10'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['MS_Score_50']=df['proximity_50_position'].apply(map_MS_score)\n",
    "df['MS_Score_avg_50']=df['MS_Score_50'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "print('Done with MS') \n",
    "\n",
    "df['LS_Score_100']=df['proximity_100_position'].apply(map_LS_score)\n",
    "df['LS_Score_avg_100']=df['LS_Score_100'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['LS_Score_10']=df['proximity_10_position'].apply(map_LS_score)\n",
    "df['LS_Score_avg_10']=df['LS_Score_10'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "\n",
    "df['LS_Score_50']=df['proximity_50_position'].apply(map_LS_score)\n",
    "df['LS_Score_avg_50']=df['LS_Score_50'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "print('Done with LS') \n",
    "\n",
    "df['ES_Score_100']=df['proximity_100_position'].apply(map_ES_score)\n",
    "df['ES_Score_avg_100']=df['ES_Score_100'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['ES_Score_10']=df['proximity_10_position'].apply(map_ES_score)\n",
    "df['ES_Score_avg_10']=df['ES_Score_10'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['ES_Score_50']=df['proximity_50_position'].apply(map_ES_score)\n",
    "df['ES_Score_avg_50']=df['ES_Score_50'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "print('Done with ES') \n",
    "\n",
    "df['HS_Score_100']=df['proximity_100_position'].apply(map_HS_score)\n",
    "df['HS_Score_avg_100']=df['HS_Score_100'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['HS_Score_10']=df['proximity_10_position'].apply(map_ES_score)\n",
    "df['HS_Score_avg_10']=df['HS_Score_10'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['HS_Score_50']=df['proximity_50_position'].apply(map_ES_score)\n",
    "df['HS_Score_avg_50']=df['HS_Score_50'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "print('Done with HS') \n",
    "\n",
    "df['US_Score_100']=df['proximity_100_position'].apply(map_US_score)\n",
    "df['US_Score_avg_100']=df['US_Score_100'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['US_Score_10']=df['proximity_10_position'].apply(map_US_score)\n",
    "df['US_Score_avg_10']=df['US_Score_10'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "df['US_Score_50']=df['proximity_50_position'].apply(map_US_score)\n",
    "df['US_Score_avg_50']=df['US_Score_50'].apply(lambda x: np.nanmean(x))\n",
    "      \n",
    "print('Done with US') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['US_Score_avg_100'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['Transparency'], prefix = 'Cat_')], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = df.groupby('Program').count().reset_index()\n",
    "program['Program Check'] = program['Number of workers'].copy()\n",
    "program = program[['Program Check', 'Program']]\n",
    "\n",
    "df = pd.merge(df, program, on = 'Program', how ='left')\n",
    "df['Program Stand'] = (df['Program Check'] - df['Program Check'].min())/(df['Program Check'].max() - df['Program Check'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = df[['Factory name', 'Version Month', 'Report No', 'Country', 'Province', 'Product type', 'Buyer 1 (%)', 'Number of workers',\n",
    "       'Daily hours (highest)', 'Weekly hours (highest)',\n",
    "       'Continuous days (most)', 'Hourly wage (lowest)_USD equ', 'Hourly wage (legal)_USD equ',\n",
    "        'Monthly take-home (lowest)_USD equ',\n",
    "       'Monthly take-home (average)_USD equ',\n",
    "       'Hourly wage (average)_USD equ',  'Paid above hourly (%)', 'Paid correctly (%)',\n",
    "       'Paid above monthly (%)', \n",
    "       'Business Ethics Findings', 'Environment Findings',\n",
    "       'Health & Safety Findings', 'Labour Standards Findings',\n",
    "       'Management Systems Findings', 'migrant worker %',\n",
    "       'Workers country of origin', 'migrant worker %', 'client_count',\n",
    "       'cert_count', 'Unauthorized Sub-Contracting', 'Union_Worker_Committee',\n",
    "       'same_language', 'Local Owner', 'Business Ethics Findings Normalized',\n",
    "       'Environment Findings Normalized',\n",
    "       'Health & Safety Findings Normalized',\n",
    "       'Labour Standards Findings Normalized',\n",
    "       'Management Systems Findings Normalized',\n",
    "       'Number of findings (total) Normalized', 'proximity_10', 'proximity_100', 'proximity_50',\n",
    "        'proximity_100_Small_factory','proximity_50_Small_factory', 'proximity_10_Small_factory',\n",
    "     'proximity_100_Medium_factory', 'proximity_50_Medium_factory', 'proximity_10_Medium_factory', \n",
    "     'proximity_100_Large_factory', 'proximity_10_Large_factory', 'proximity_50_Large_factory',\n",
    "      'Wage_Score_avg_100', 'Wage_Score_avg_10', 'Wage_Score_avg_50',\n",
    "       'BE_Score_avg_100', 'BE_Score_avg_10',  'BE_Score_avg_50',\n",
    "       'MS_Score_avg_100', 'MS_Score_avg_10', 'MS_Score_avg_50',\n",
    "       'LS_Score_avg_100',  'LS_Score_avg_10', 'LS_Score_avg_50',\n",
    "       'ES_Score_avg_100',  'ES_Score_avg_10', 'ES_Score_avg_50',\n",
    "       'HS_Score_avg_100', 'HS_Score_avg_10', 'HS_Score_avg_50', \n",
    "       'US_Score_avg_100',  'US_Score_avg_10','US_Score_avg_50',\n",
    "        'Program', 'Cat__Falsifying','Cat__Inconclusive', 'Cat__Transparent', 'Cat__Unknown', 'Program Stand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_exp.to_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\India_ML_231019.xlsx')\n",
    "# df_add.to_excel(r'C:\\Users\\filippo.sebastio\\OneDrive - ELEVATE\\Data\\2015-2018\\China_ML_111019_Add_Prox.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "341.719px",
    "left": "1227.45px",
    "right": "20px",
    "top": "106.974px",
    "width": "428.182px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
